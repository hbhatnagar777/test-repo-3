# -*- coding: utf-8 -*-

# --------------------------------------------------------------------------
# Copyright Commvault Systems, Inc.
# See LICENSE.txt in the project root for
# license information.
# --------------------------------------------------------------------------

""" Main file for helping testcases validate DR functionalities for Failback
Failback
    Methods:
        validate_sync_status():     Validates the sync status and returns True/False
        validate_power_state():     Validates whether the power state of the VMs is correct
        validate_snapshot():        Validates that the failback snapshot is deleted on source VM
"""
from cvpysdk.drorchestration.blr_pairs import BLRPair
from cvpysdk.recovery_targets import RecoveryTarget
from cvpysdk.subclients.virtualserver.livesync.vsa_live_sync import LiveSyncVMPair

from AutomationUtils.database_helper import CommServDatabase
from DROrchestration.Core._dr_validation import _DROrchestrationValidation
from VirtualServer.VSAUtils import VirtualServerHelper
from VirtualServer.VSAUtils.VMHelpers.AzureVM import AzureVM
from VirtualServer.VSAUtils.VMHelpers.GoogleCloudVM import GoogleCloudVM


class FailbackPeriodic(_DROrchestrationValidation):
    """ This class is used to provide utility functions for validating failback process"""
    SOURCE_GUID_CHANGE_VENDORS = (AzureVM, GoogleCloudVM)

    def __init__(self, vm_pair_object: LiveSyncVMPair or BLRPair,
                 source_auto_instance: VirtualServerHelper.AutoVSAVSInstance,
                 destination_auto_instance: VirtualServerHelper.AutoVSAVSInstance,
                 vm_options: dict,
                 job_type: str = '',
                 recovery_target: RecoveryTarget = None,
                 csdb: CommServDatabase = None):
        """ Override the init to store the replication pair before failback """
        super().__init__(vm_pair_object,
                         source_auto_instance,
                         destination_auto_instance,
                         vm_options,
                         job_type,
                         recovery_target,
                         csdb)

        self._old_replication_pair = None

    def pre_validate_sync_status(self):
        """ Validates the sync status of the live sync pair before the failback operation """
        self.refresh()
        if self._live_sync_pair.status != 'SYNC_DISABLED':
            raise Exception(f"{self._live_sync_pair} is not in 'Sync disabled' status")
        self.log.info('Sync status for VM pair %s verified before operation', str(self._live_sync_pair))

    def validate_sync_status(self):
        """ Validates the sync status of the live sync pair after failback is complete """
        self.refresh()
        if self._live_sync_pair.status not in ['IN_SYNC', 'NEEDS_SYNC']:
            raise Exception(f"{self._live_sync_pair} is not in 'In sync' or 'Need sync' status after failback")
        self.log.info('Sync status for VM pair %s verified after operation', str(self._live_sync_pair))

    def validate_failover_status(self):
        """ Validates the failover status of the live sync pair is Failback complete """
        self.refresh()
        # Check if failover status is not set or is set to failback complete
        if self._live_sync_pair._failover_status and self._live_sync_pair.failover_status != 'FAILBACK_COMPLETE':
            raise Exception(f"{self._live_sync_pair} does not have 'Failback complete' failover status")
        self.log.info('Failover status for VM pair %s verified after operation', str(self._live_sync_pair))

    def validate_power_state(self):
        """ Validates the power state of the source and destination VM """
        self.refresh_vm(source=True, basic_only=True)
        if not self.source_vm.vm.is_powered_on():
            raise Exception(f"Source VM [{self._source_vm.vm_name}] is not powered on even after failback")
        # Note : Power state validation is skipped for Warm Sync and DVDF (Reason : DRVM does not exist)
        if not (self.is_dvdf_enabled or self.is_warm_sync_enabled):
            self.refresh_vm(source=False, basic_only=True)
            if self.destination_vm.vm.is_powered_on():
                raise Exception(f"Destination VM [{self._destination_vm.vm_name}] is powered on even after failback")
        self.log.info('Power states for VM pair %s verified after operation', str(self._live_sync_pair))

    def validate_snapshot(self, **kwargs):
        """ Validates that the integrity snapshot generated by failover is removed """
        # Validate DR VM's integrity snapshot creation and failover snapshot doesn't exist
        self.refresh_vm(source=True, basic_only=True)
        self.source_vm.validate_snapshot(integrity_check=False, **kwargs)

        if not (self.is_dvdf_enabled or self.is_warm_sync_enabled):
            self.refresh_vm(source=False, basic_only=True)
            self.destination_vm.validate_snapshot(integrity_check=True, **kwargs)

    def validate_dvdf(self):
        """ Validates that the deployed VM after failback is not present """
        # Validate that the DVDF entities do NOT exist on destination VM
        try:
            self.destination_vm.validate_dvdf_on_failover()
        except:
            self.log.info('DVDF entities for VM pair %s verified after failback after failback',
                          str(self._live_sync_pair))
        else:
            raise Exception(f"DVDF entities still exist for {self._live_sync_pair} on destination"
                            f" hypervisor even after failback")

    def validate_replication_guid(self, after_failback=True):
        """ Validates that the replication GUID for source VM changed after failback"""
        # Skip validation if replication is continuous or not from cloud hypervisors
        # TODO: Add check in subtask XML
        # Currently, the source VM GUID only changes when source VM is Azure/GCP
        if not (isinstance(self.vm_pair, LiveSyncVMPair)
                and isinstance(self._source_vm, self.SOURCE_GUID_CHANGE_VENDORS)):
            return
        if after_failback:
            if self._old_replication_pair.get('sourceGuid') == self.vm_pair.source_vm_guid:
                raise Exception("Source VM's GUID did not change in replication monitor after failback")

            if int(self._old_replication_pair.get('replicationId')) != int(self.vm_pair.vm_pair_id):
                raise Exception(f"VM pair ID changed after failback from "
                                f"{self._old_replication_pair.get('replicationId')} to "
                                f"{self.vm_pair.vm_pair_id}")

            self.csdb.execute(f"select * from APP_VSAReplication where sourceGuid = '{self.vm_pair.source_vm_guid}'"
                              f" and taskId = '{self.vm_pair.task_id}'")
            replication_rows = self.csdb.fetch_all_rows(named_columns=True)
            if len(replication_rows) != 1:
                raise Exception(f"Multiple rows found in APP_VSAReplication with IDs: "
                                f"{[row.get('id')for row in replication_rows]}")
        else:
            self._old_replication_pair = self.vm_pair._properties

    def evaluate_backup_proxy(self):
        """ Validates the proxy used for last synced backup job for the vm pairs """

        # last synced Backup vm list
        backup_job = str(self.vm_pair.last_synced_backup_job)
        backup_job_obj = self._commcell_object.job_controller.get(backup_job)
        backup_vm_list = backup_job_obj.get_vm_list()

        # validates backup job proxy used for each vm is one from destination proxies list
        self.log.info("Validating last synced backup job proxy")
        for vm in backup_vm_list:
            bkp_proxy = vm.get("Agent")
            if bkp_proxy:
                if bkp_proxy.lower() not in self.destination_proxy_list:
                    raise Exception(f"VM : {vm['vmName']} backup used proxy: {bkp_proxy}, expected"
                                    f"proxy to be used: {self.destination_proxy_list}")
            else:
                raise Exception("Backup proxy is None")
        self.log.info("Backup job %s proxies validated successfully", backup_job)

    def evaluate_replication_proxy(self):
        """ Validates the proxy used for last successful replication job for the vm pairs """

        # last successful Replication vm list
        rep_job = str(self.vm_pair.latest_replication_job)
        rep_job_obj = self._commcell_object.job_controller.get(rep_job)
        rep_vm_list = rep_job_obj.get_vm_list()

        # validates replication job proxy used for each vm is one from source hypervisor proxies list
        self.log.info("Validating last successful replication job proxy")
        for vm in rep_vm_list:
            rep_proxy = vm.get("Agent")
            if rep_proxy:
                if rep_proxy.lower() not in self.source_proxy_list:
                    raise Exception(f"VM {vm['destinationVMName']} replication used proxy {rep_proxy} "
                                    f"not present in proxy list : {self.source_proxy_list}")
            else:
                raise Exception("Replication proxy is None")
        self.log.info("Replication job %s proxy validated successfully", rep_job)


class FailbackContinuous(_DROrchestrationValidation):
    def validate_snapshot(self):
        """ Validates that the snapshot generated by failover is removed """
        pass

    def pre_validate_sync_status(self):
        """ Validates the sync status of the live sync pair before the failback operation """
        pass

    def validate_sync_status(self):
        """ Validates the sync status of the live sync pair """
        pass

    def validate_failover_status(self):
        """ Validates the failover status of the live sync pair is Failback complete """
        pass

    def validate_power_state(self):
        """ Validates the power state of the source and destination VM """
        pass
